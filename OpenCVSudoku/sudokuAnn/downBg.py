import requests
import os
import time
import random
from urllib.parse import quote
from PIL import Image
import io
import cloudscraper
import re

from multiprocessing import Pool, Manager, Lock
import json
from datetime import datetime
import hashlib

def string_to_md5(text):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºMD5å“ˆå¸Œå€¼"""
    # åˆ›å»ºMD5å¯¹è±¡
    md5_hash = hashlib.md5()
    # æ›´æ–°å“ˆå¸Œå¯¹è±¡ï¼ˆéœ€è¦å°†å­—ç¬¦ä¸²ç¼–ç ä¸ºbytesï¼‰
    md5_hash.update(text.encode('utf-8'))
    # è·å–åå…­è¿›åˆ¶å“ˆå¸Œå€¼
    return md5_hash.hexdigest()
import multiprocessing as mp
from multiprocessing import Pool, set_start_method
import os
import cloudscraper
from PIL import Image
import io
import re
from urllib.parse import quote
import hashlib
import time
import random

# è®¾ç½®å¯åŠ¨æ–¹æ³•
try:
    set_start_method('spawn', force=True)
except RuntimeError:
    pass

def download_worker(task_data):
    """ç‹¬ç«‹çš„å·¥ä½œå‡½æ•°"""
    img_url, query, width, height, save_dir,err_domains, index = task_data
    
    try:
        domain = img_url.split('/')[2]
        max_err_count = 3
        if domain in err_domains and err_domains[domain] >= max_err_count:
            print(f"è·³è¿‡é«˜é”™è¯¯ç‡åŸŸå: {domain} cnt:{err_domains[domain]}/{max_err_count}")
            err_domains[domain] = err_domains.get(domain, 0) + 1
            return {'status': 'failed', 'error': f'è·³è¿‡é«˜é”™è¯¯ç‡åŸŸå cnt:{err_domains[domain]}/{max_err_count} for {img_url} '}
        # ä½¿ç”¨MD5é¿å…æ–‡ä»¶åå†²çª
        url_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
        filename = f"{query}_{width}x{height}_{url_hash}.jpg"
        filepath = os.path.join(save_dir, filename)
        if (index+1) % 50 == 0:
            print(f"è¿›åº¦: {index+1} ä¸‹è½½ä¸­... {filename}")
        # è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶
        if os.path.exists(filepath):
            print(f"æ–‡ä»¶å·²å­˜åœ¨: {filepath}")
            return {'status': 'skipped', 'filename': filename}
        
        scraper = cloudscraper.create_scraper()
        proxies = {
            'http': 'socks5://127.0.0.1:10808',
            'https': 'socks5://127.0.0.1:10808'
        }
        scraper.headers.update({
            'User-Agent': random.choice([
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            ])
        })
        
        img_response = scraper.get(img_url, timeout=15,proxies=proxies)
        
        if img_response.status_code == 200:
            img = Image.open(io.BytesIO(img_response.content))
            if img.size != (width, height):
                img = img.resize((width, height), Image.Resampling.LANCZOS)
            
            if img.mode != 'RGB':
                img = img.convert('RGB')
            
            img.save(filepath, 'JPEG', quality=90)
            time.sleep(random.uniform(0.5, 2))
            
            return {'status': 'success', 'filename': filename}
        else:
            err_domains[domain] = err_domains.get(domain, 0) + 1
            return {'status': 'failed', 'error': f'HTTP {img_response.status_code} for {img_url}'}
            
    except Exception as e:
        err_domains[domain] = err_domains.get(domain, 0) + 1
        return {'status': 'failed', 'error': str(e) + f' for {img_url}'}

class ParallelImageDownloader:
    def __init__(self, max_processes=None):
        self.max_processes = max_processes or min(mp.cpu_count(), 6)
        self.manager = Manager()
        self.err_domains = self.manager.dict()  # é”™è¯¯åŸŸåç»Ÿè®¡
    
    def get_image_urls(self, query, count, max_pages=0):
        """æ›´æ™ºèƒ½çš„æ»šåŠ¨åŠ è½½ï¼Œæ”¯æŒå¤šç§åˆ†é¡µç­–ç•¥"""
        if not max_pages or max_pages <= 0:
            max_pages = (count // 35) * 1.3
        try:
            scraper = cloudscraper.create_scraper()
            query_encoded = quote(query)
            all_image_urls = set()  # ä½¿ç”¨é›†åˆè‡ªåŠ¨å»é‡
            page = 1
            page_count = 35
            zero_url_count = 0
            
            print(f"ğŸ” æœç´¢: '{query}'ï¼Œç›®æ ‡: {count} å¼ å›¾ç‰‡")
            
            while len(all_image_urls) < count and page <= max_pages:
                # å¤šç§URLæ ¼å¼å°è¯•
                urls_to_try = [
                    f"https://www.bing.com/images/search?q={query_encoded}&first={(page-1)*page_count}&count={page_count}",
                    f"https://www.bing.com/images/search?q={query_encoded}&form=HDRSC2&first={(page-1)*page_count}&count={page_count}",
                    f"https://www.bing.com/images/search?q={query_encoded}&qs=HS&form=QBIR&first={(page-1)*page_count}&count={page_count}"
                ]
                
                page_success = False
                for url in urls_to_try:
                    try:
                        response = scraper.get(url, timeout=15)
                        if response.status_code != 200:
                            continue
                        
                        # å¤šç§åŒ¹é…æ¨¡å¼
                        patterns = [
                            r'murl&quot;:&quot;(https?://[^&]+?\.(?:jpg|jpeg|png|webp))',
                            r'src=&quot;(https?://[^&]+?\.(?:jpg|jpeg|png|webp))',
                            r'imgurl&quot;:&quot;(https?://[^&]+?\.(?:jpg|jpeg|png|webp))'
                        ]
                        
                        page_urls = set()
                        for pattern in patterns:
                            found_urls = re.findall(pattern, response.text)
                            page_urls.update(found_urls)
                        
                        if page_urls:
                            new_count = len(page_urls - all_image_urls)
                            all_image_urls.update(page_urls)
                            if new_count == 0:
                                zero_url_count += 1
                            else:
                                zero_url_count = 0
                            
                            print(f"ğŸ“„ ç¬¬ {page} é¡µæ‰¾åˆ° {new_count} å¼ æ–°å›¾ç‰‡ï¼Œæ€»è®¡: {len(all_image_urls)}")
                            page_success = True
                            break
                            
                    except Exception as e:
                        continue
                if zero_url_count >= 4:
                    print(f"âš ï¸  ç¬¬ {page} é¡µæ²¡æœ‰æ–°å›¾ç‰‡ï¼Œå¯èƒ½æ˜¯é‡å¤å†…å®¹")
                    break
                
                if not page_success:
                    print(f"âš ï¸  ç¬¬ {page} é¡µåŠ è½½å¤±è´¥")
                
                # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡ï¼Œæå‰é€€å‡º
                if len(all_image_urls) >= count:
                    break
                
                page += 1
                time.sleep(random.uniform(0.5, 3))  # éšæœºå»¶è¿Ÿ
            
            result = list(all_image_urls)[:count]
            print(f"ğŸ¯ æœç´¢å®Œæˆ: æ‰¾åˆ° {len(result)} å¼ å›¾ç‰‡")
            return result
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def download_parallel(self, query, count=20, width=512, height=512, save_dir='downloads'):
        """ä¸»ä¸‹è½½æ–¹æ³•"""
        os.makedirs(save_dir, exist_ok=True)
        
        # è·å–å›¾ç‰‡URL
        image_urls = self.get_image_urls(query, count)
        if not image_urls:
            print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡é“¾æ¥")
            return {'success': 0, 'failed': count}
        
        print(f"âœ… æ‰¾åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡é“¾æ¥")
        
        # å‡†å¤‡ä»»åŠ¡
        tasks = []
        for i, img_url in enumerate(image_urls[:count]):
            tasks.append((img_url, query, width, height, save_dir,self.err_domains, i))
        
        # ä½¿ç”¨è¿›ç¨‹æ± 
        success_count = 0
        failed_count = 0
        
        with Pool(processes=self.max_processes) as pool:
            results = pool.map(download_worker, tasks)
            
            for result in results:
                if result['status'] == 'success' or result['status'] == 'skipped':
                    success_count += 1
                    # print(f"âœ… æˆåŠŸ: {result['filename']}")
                else:
                    failed_count += 1
                    print(f"âŒ å¤±è´¥: {result['error']}")
        
        return {'success': success_count, 'failed': failed_count}



# åŸºæœ¬ä½¿ç”¨
if __name__ == "__main__":
    
    # æ–¹æ³•3: ä½¿ç”¨ç±»ç®¡ç†

    downloader = ParallelImageDownloader(max_processes=4)
    save_dir = "./img/bg"
    # query="background book"
    # # query="background ä¹¦æ¡Œ"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    # query="background ä¹¦æ¡Œ"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    # query="background city"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    
    # query="background paper"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    # query="background scenery"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    # query="background room"
    # stats = downloader.download_parallel(
    #     query=query,
    #     count=3000,
    #     width=1920,
    #     height=1080,
    #     save_dir=save_dir
    # )
    query="background æŠ¥çº¸"
    stats = downloader.download_parallel(
        query=query,
        count=3000,
        width=1920,
        height=1080,
        save_dir=save_dir
    )
    print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")